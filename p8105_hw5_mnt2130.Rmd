---
title: "p8105_hw5_mnt2130"
author: "Mindy Tran"
date: "2022-11-16"
output: github_document
---
```{r setup, include = F}
library(tidyverse)
library(p8105.datasets)
library(httr)
library(viridis)
library(purrr)
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```
### Problem 1

The code chunk below imports all the individual CSVs contained in the zip file by creating a dataframe that includes all the files in that folder and the path to each file. Next, I `map` over paths and import the data through the  `read_csv` function. Finally, I `unnest` the result of `map`. 

```{r}
full_df = 
  tibble(
    files = list.files("data/zip_data/"),
    path = str_c("data/zip_data/", files)
  ) %>% 
  mutate(data = map(path, read_csv)) %>% 
  unnest(cols = c(data))
```

The result of the previous code chunk is untidy. The data is in wide format instead of long and some important variables are included as parts of other variables. The code chunk below cleans the data using string manipulations on the file, converting from wide to long, and selecting relevant variables. 

```{r}
tidy_df = 
  full_df %>% 
  mutate(
    files = str_replace(files, ".csv", ""),
    group = str_sub(files, 1, 3)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "outcome",
    names_prefix = "week_") %>% 
  mutate(week = as.numeric(week)) %>% 
  select(group, subj = files, week, outcome)
```

Next, the following code chunk makes a spaghetti plot showing observations on each subject over time. 

```{r plot}
tidy_df %>% 
  ggplot(aes(x = week, y = outcome, group = subj, color = group)) + 
  geom_point() + 
  geom_path() + 
  facet_grid(~group)
```

This plot suggests high within-subject correlation – subjects who start
above average end up above average, and those that start below average
end up below average. Subjects in the control group generally don’t
change over time, but those in the experiment group increase their
outcome measure in a roughly linear way with roughly an upward trend.


### Problem 2

The following code imports the data in CSV form and cleans the data. 
```{r clean}
raw_homicide_df =
  read_csv("./data/homicide-data.csv", na = c("", "Unknown"), show_col_types = FALSE)
homicide_df = raw_homicide_df %>% 
  mutate(
    city_state = str_c(city, state),
    resolution = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest" ~ "unsolved",
      disposition == "Closed by arrest" ~ "solved"
      )) %>% 
  relocate(city_state) %>% 
  filter(city_state != "TulsaAL")
```

The homicide data set from the Washington Post contains `r nrow(raw_homicide_df)` observations with `r ncol(raw_homicide_df)` variables. The key variables of interest included this data set include `r names(raw_homicide_df %>% select(c(8, 9, 12)))`.

With the following code chunk I created a city_state variable (e.g. “Baltimore, MD”) and then summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”).For the city of Baltimore, MD, I used the `prop.test` function to estimate the proportion of homicides that are unsolved, saved the output as an R object named baltimore_test and applied the broom::tidy to this object to pull the estimated proportion and confidence intervals from the data frame.

```{r summary}
baltimore_df = 
  homicide_df %>% 
  filter(city_state == "BaltimoreMD")
baltimore_summary = 
  baltimore_df %>% 
  summarize(
    unsolved = sum(resolution == "unsolved"),
    n = n()
  )
baltimore_test =
prop.test(
  x = baltimore_summary %>% pull(unsolved),
  n = baltimore_summary %>% pull(n))
baltimore_test %>% 
  broom::tidy()
```
The code chunk below runs a  prop.test for each of the cities in the dataset, and extracts both the proportion of unsolved homicides and the confidence interval for each within a “tidy” pipeline, makes use of purrr::map, purrr::map2, list columns and unnest  to create a tidy dataframe with estimated proportions and CIs for each city:

```{r prop test}
prop_test_function = function(city_df) {
  
  city_summary =
    city_df %>%
    summarize(
      unsolved = sum(resolution == "unsolved"),
      n = n()
    )
  city_test =
    prop.test(
      x = city_summary %>% pull(unsolved),
      n = city_summary %>% pull(n))
  
  return(city_test)
}
```
```{r results}
results_df = 
  homicide_df %>% 
  nest(data = uid:resolution) %>% 
  mutate(
    test_results = map(data, prop_test_function),
    tidy_results = map(test_results, broom::tidy)
  ) %>% 
  select(city_state, tidy_results) %>% 
  unnest(tidy_results) %>% 
  select(city_state, estimate, starts_with("conf"))
```
Finally, we create a plot that shows the estimates and CIs for each city. 

```{r city CI plot}
results_df %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  labs(
    title = "Estimates and CIs for Unsolved Homicides by City",
    x = "City",
    y = "Proportion of Unsolved Homicides ",
    caption = "Data Obtained from Washington Post")
```

