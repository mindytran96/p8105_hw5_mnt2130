---
title: "p8105_hw5_mnt2130"
author: "Mindy Tran"
date: "2022-11-16"
output: github_document
---
```{r setup, include = F}
library(tidyverse)
library(p8105.datasets)
library(httr)
library(viridis)
library(purrr)
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```
### Problem 1

The code chunk below imports all the individual CSVs contained in the zip file by creating a dataframe that includes all the files in that folder and the path to each file. Next, I `map` over paths and import the data through the  `read_csv` function. Finally, I `unnest` the result of `map`. 

```{r}
full_df = 
  tibble(
    files = list.files("data/zip_data/"),
    path = str_c("data/zip_data/", files)
  ) %>% 
  mutate(data = map(path, read_csv)) %>% 
  unnest(cols = c(data))
```

The result of the previous code chunk is untidy. The data is in wide format instead of long and some important variables are included as parts of other variables. The code chunk below cleans the data using string manipulations on the file, converting from wide to long, and selecting relevant variables. 

```{r}
tidy_df = 
  full_df %>% 
  mutate(
    files = str_replace(files, ".csv", ""),
    group = str_sub(files, 1, 3)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "outcome",
    names_prefix = "week_") %>% 
  mutate(week = as.numeric(week)) %>% 
  select(group, subj = files, week, outcome)
```

Next, the following code chunk makes a spaghetti plot showing observations on each subject over time. 

```{r plot}
tidy_df %>% 
  ggplot(aes(x = week, y = outcome, group = subj, color = group)) + 
  geom_point() + 
  geom_path() + 
  facet_grid(~group)
```

This plot suggests high within-subject correlation – subjects who start
above average end up above average, and those that start below average
end up below average. Subjects in the control group generally don’t
change over time, but those in the experiment group increase their
outcome measure in a roughly linear way with roughly an upward trend.


### Problem 2

The following code imports the data in CSV form and cleans the data. 
```{r clean}
raw_homicide_df =
  read_csv("./data/homicide-data.csv", na = c("", "Unknown"), show_col_types = FALSE)
homicide_df = raw_homicide_df %>% 
  mutate(
    city_state = str_c(city, state),
    resolution = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest" ~ "unsolved",
      disposition == "Closed by arrest" ~ "solved"
      )) %>% 
  relocate(city_state) %>% 
  filter(city_state != "TulsaAL")
```

The homicide data set from the Washington Post contains `r nrow(raw_homicide_df)` observations with `r ncol(raw_homicide_df)` variables. The key variables of interest included this data set include `r names(raw_homicide_df %>% select(c(8, 9, 12)))`.

With the following code chunk I created a city_state variable (e.g. “Baltimore, MD”) and then summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”).For the city of Baltimore, MD, I used the `prop.test` function to estimate the proportion of homicides that are unsolved, saved the output as an R object named baltimore_test and applied the broom::tidy to this object to pull the estimated proportion and confidence intervals from the data frame.

```{r summary}
baltimore_df = 
  homicide_df %>% 
  filter(city_state == "BaltimoreMD")
baltimore_summary = 
  baltimore_df %>% 
  summarize(
    unsolved = sum(resolution == "unsolved"),
    n = n()
  )
baltimore_test =
prop.test(
  x = baltimore_summary %>% pull(unsolved),
  n = baltimore_summary %>% pull(n))
baltimore_test %>% 
  broom::tidy()
```
The code chunk below runs a  prop.test for each of the cities in the dataset, and extracts both the proportion of unsolved homicides and the confidence interval for each within a “tidy” pipeline, makes use of purrr::map, purrr::map2, list columns and unnest  to create a tidy dataframe with estimated proportions and CIs for each city:

```{r prop test}
prop_test_function = function(city_df) {
  
  city_summary =
    city_df %>%
    summarize(
      unsolved = sum(resolution == "unsolved"),
      n = n()
    )
  city_test =
    prop.test(
      x = city_summary %>% pull(unsolved),
      n = city_summary %>% pull(n))
  
  return(city_test)
}
```
```{r results}
results_df = 
  homicide_df %>% 
  nest(data = uid:resolution) %>% 
  mutate(
    test_results = map(data, prop_test_function),
    tidy_results = map(test_results, broom::tidy)
  ) %>% 
  select(city_state, tidy_results) %>% 
  unnest(tidy_results) %>% 
  select(city_state, estimate, starts_with("conf"))
```
Finally, we create a plot that shows the estimates and CIs for each city. 

```{r city CI plot}
results_df %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  labs(
    title = "Estimates and CIs for Unsolved Homicides by City",
    x = "City",
    y = "Proportion of Unsolved Homicides ",
    caption = "Data Obtained from Washington Post")
```

### Problem 3
The following code defines a function that runs a one-sample t-test and generates mu_hat (the population mean) and the p-value from a dataset generated from random numbers from a normal distribution with a sample size of 30 and a standard deviation (sigma) equal to 5. 
```{r}
t_test = function(mu, samp_size = 30, sigma = 5) {
  sim_data = 
    tibble(
      x = rnorm(n = samp_size, mean = mu, sd = sigma)
    )
  
   sim_data %>% 
     summarize(
       mu_hat = t.test(x) %>% broom::tidy() %>% pull(estimate),
       p_value = t.test(x) %>% broom::tidy() %>% pull(p.value)
       
    )
}
```
Next, the code below creates a dataframe with 5000 datasets with a sample size of 30 and standard deviation of 5. Then, it performs a one-sample t-test to generate the population mean and p-value and saves the results in another dataframe named test_results. 

```{r cache=T}
ttest_results = tibble(
  mu = c(0,1,2,3,4,5,6)) %>% 
  mutate(
    output_lists = map(.x = mu, ~rerun(5000, t_test(.x))),
    estimate_df = map(output_lists, bind_rows)
  ) %>% 
  select(-output_lists) %>% 
  unnest(estimate_df) 
```
The following code creates a plot that shows the proportion of times the null hypothesis was rejected (the power of the test) on the y axis and the true value of μ on the x axis:
```{r plot 1, message= FALSE}
ttest_results %>% 
  group_by(mu) %>% 
  summarize(
    total = n(), 
    num_rejected = sum(p_value < 0.05), 
    prop_rejected = num_rejected/total
  ) %>% 
  ggplot(aes(x = mu, y = prop_rejected)) + 
  geom_point() + 
  geom_smooth(se = F) + 
  labs(
    title = "Power of Test Vs. Effect Size",
    x = "True Population Mean",
    y = "Power of Test"
  )
```

Next, this code makes a plot showing the average estimate of μ̂  on the y axis and the true value of μ on the x axis. It also overlaysthe average estimate of μ̂  only in samples for which the null was rejected on the y axis and the true value of μ on the x axis onto the first plot. 

```{r, message= FALSE, fig.width=8, fig.height=12}
ttest_results2 = ttest_results %>% 
  filter(p_value < 0.05) %>% 
  group_by(mu) %>% 
  summarize(
    mean_mu_hat = mean(mu_hat)
  ) 

ttest_results %>% 
  group_by(mu) %>% 
  summarize(
    mean_mu_hat = mean(mu_hat)
  ) %>% 
  ggplot(aes(y = mean_mu_hat, x = mu)) + 
  geom_line() + 
  geom_point() +
    geom_line(data = ttest_results2, color = "blue", linetype = "dashed") +
    geom_point(data = ttest_results2, color = "blue") + 
  labs(
    title = "Mean of Estimate Vs. True Mean",
    x = "True Population Mean",
    y = "Mean of Estimate",
    caption = "
    *Blue dashed line shows those tests with rejected null hypothesis \n
            **Solid black line shows all tests (null hypothesis rejected + null hypothesis not rejected)"
  ) 
```